{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6030edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts= 5,\n",
    "    exp_base = 0.2,\n",
    "    initial_delay = 0.5,\n",
    "    http_status_codes = [500, 502, 503, 504],   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9b8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"API keys loaded\" if LLAMA_CLOUD_API_KEY and GEMINI_API_KEY else \"✗ Missing API keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d39366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILINGS= [\n",
    "    {\"id\":\"1\",\"company\":\"Apple\",\"year\":\"2023\",\"path\": r\"C:\\Users\\rushy\\Downloads\\FINBOT\\GenAI_FInBot\\NOV_2023.pdf\"},\n",
    "    {\"id\":\"2\",\"company\":\"Tesla\",\"year\":\"2023\",\"path\": r\"C:\\Users\\rushy\\Downloads\\FINBOT\\GenAI_FInBot\\Tesla_2023.pdf\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a70e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = [\n",
    "    #{\"provider\":\"OpenAI\",\"model_name\":\"text-embedding-3-small\",\"api_key_env\":\"OPENAI_API_KEY\",\"dimensions\":1536},\n",
    "    {\"provider\":\"Google\",\"model_name\":\"text-embedding-004\",\"api_key_env\":\"GEMINI_API_KEY\",\"dimensions\":768},\n",
    "    {\"provider\":\"Local\",\"model_name\":\"all-MiniLM-L6-v2\",\"api_key_env\":None,\"dimensions\":384},\n",
    "    {\"provider\":\"Local\",\"model_name\":\"intfloat/e5-large-v2\",\"api_key_env\":None,\"dimensions\":1024},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab735a",
   "metadata": {},
   "source": [
    "Need to check\n",
    "1. Latency p90\n",
    "2. Recall p50\n",
    "3. Semantic Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe0b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65267b9",
   "metadata": {},
   "source": [
    "Pipleline Skeleton\n",
    "1. pages = extract_pages_text(pdf_paths)\n",
    "2. tables = extract_tables(pdf_paths)\n",
    "3. docs = build_docs(pages,tables)\n",
    "4. chunks = chuk_docs(docs)\n",
    "5. index(chuks,embedding_model_cfg)\n",
    "6. retrieve(query) -> context\n",
    "7. generate(context,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcdaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(text:str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).strip()\n",
    "\n",
    "\n",
    "def extract_pages_from_filings(filings):\n",
    "    all_pages = []\n",
    "\n",
    "    for filing in filings:\n",
    "        doc = fitz.open(filing['path'])\n",
    "        source_name = Path(filing['path']).name\n",
    "\n",
    "        for page_idx,page in enumerate(doc,start=1):\n",
    "            text = page.get_text(\"text\")\n",
    "            all_pages.append({\n",
    "                \"filing_id\":filing['id'],\n",
    "                \"company\": filing['company'],\n",
    "                \"year\":filing['year'],\n",
    "                \"source\": source_name,\n",
    "                \"page\":page_idx,\n",
    "                \"text\":clean_text(text),\n",
    "            })\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    return all_pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa1d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filing_id': '2', 'company': 'Tesla', 'year': '2023', 'source': 'Tesla_2023.pdf', 'page': 21, 'text': 'Our information technology systems or data or those of our service providers or customers or users could be subject to cyber\\nattacks or other security incidents which could result in data breaches intellectual property theft claims litigation regulatory \\ninvestigations significant liability reputational damage and other adverse consequences \\nWe continue to expand our information technology systems as our operations grow such as product data management procurement inventory \\nmanagement production planning and execution sales service and logistics dealer management financial tax and regulatory compliance systems \\nThis includes the implementation of new internally developed systems and the deployment of such systems in the US and abroad While we maintain \\ninformation technology measures designed to protect us against intellectual property theft data breaches sabotage and other external or internal \\ncyberattacks or misappropriation our systems and those of our service providers are potentially vulnerable to malware ransomware viruses denialof\\nservice attacks phishing attacks social engineering computer hacking unauthorized access exploitation of bugs defects and vulnerabilities \\nbreakdowns damage interruptions system malfunctions power outages terrorism acts of vandalism security breaches security incidents \\ninadvertent or intentional actions by employees or other third parties and other cyberattacks \\nTo the extent any security incident results in unauthorized access or damage to or acquisition use corruption loss destruction alteration or \\ndissemination of our data including intellectual property and personal information or our products or vehicles or for it to be believed or reported that \\nany of these occurred it could disrupt our business harm our reputation compel us to comply with applicable data breach notification laws subject us \\nto time consuming distracting and expensive litigation regulatory investigation and oversight mandatory corrective action require us to verify the \\ncorrectness of database contents or otherwise subject us to liability under laws regulations and contractual obligations including those that protect \\nthe privacy and security of personal information This could result in increased costs to us and result in significant legal and financial exposure andor \\nreputational harm \\nWe also rely on service providers and similar incidents relating to their information technology systems could also have a material adverse effect \\non our business There have been and may continue to be significant supply chain attacks Our service providers including our workforce management \\nsoftware provider have been subject to ransomware and other security incidents and we cannot guarantee that our or our service providers systems \\nhave not been breached or that they do not contain exploitable defects bugs or vulnerabilities that could result in a security incident or other \\ndisruption to our or our service providers systems Our ability to monitor our service providers security measures is limited and in any event \\nmalicious third parties may be able to circumvent those security measures \\nFurther the implementation maintenance segregation and improvement of these systems require significant management time support and \\ncost and there are inherent risks associated with developing improving and expanding our core systems as well as implementing new systems and \\nupdating current systems including disruptions to the related areas of business operation These risks may affect our ability to manage our data and \\ninventory procure parts or supplies or manufacture sell deliver and service products adequately protect our intellectual property or achieve and \\nmaintain compliance with or realize available benefits under tax laws and other applicable regulations \\nMoreover if we do not successfully implement maintain or expand these systems as planned our operations may be disrupted our ability to \\naccurately andor timely report our financial results could be impaired and deficiencies may arise in our internal control over financial reporting which \\nmay impact our ability to certify our financial results Moreover our proprietary information including intellectual property and personal information \\ncould be compromised or misappropriated and our reputation may be adversely affected If these systems or their functionality do not operate as we \\nexpect them to we may be required to expend significant resources to make corrections or find alternative sources for performing these functions\\nAny unauthorized control or manipulation of our products systems could result in loss of confidence in us and our products\\nOur products contain complex information technology systems For example our vehicles and energy storage products are designed with builtin \\ndata connectivity to accept and install periodic remote updates from us to improve or update their functionality While we have implemented security \\nmeasures intended to prevent unauthorized access to our information technology networks our products and their systems malicious entities have \\nreportedly attempted and may attempt in the future to gain unauthorized access to modify alter and use such networks products and systems to gain \\ncontrol of or to change our products functionality user interface and performance characteristics or to gain access to data stored in or generated by \\nour products We encourage reporting of potential vulnerabilities in the security of our products through our security vulnerability reporting policy and \\nwe aim to remedy any reported and verified vulnerability However there can be no assurance that any vulnerabilities will not be exploited before they \\ncan be identified or that our remediation efforts are or will be successful\\nAny unauthorized access to or control of our products or their systems or any loss of data could result in legal claims or government \\ninvestigations In addition regardless of their veracity reports of unauthorized access to our products their systems or data as well as other factors \\nthat may result in the perception that our products their systems or data are capable of being hacked may harm our brand prospects and operating \\nresults We have been the subject of such reports in the past\\n21'}\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "pages = extract_pages_from_filings(FILINGS)\n",
    "\n",
    "print(pages[100])\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aec7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_tables_from_filings(filings,*,max_tables_per_page=None):\n",
    "    all_tables = []\n",
    "    for filing in filings:\n",
    "        pdf_path = filing['path']\n",
    "        source_name = Path(pdf_path).name\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_idx,page in enumerate(pdf.pages,start=1):\n",
    "                tables = page.extract_tables()\n",
    "\n",
    "                if not tables:\n",
    "                    continue\n",
    "\n",
    "                if max_tables_per_page is not None:\n",
    "                    tables = tables[:max_tables_per_page]\n",
    "\n",
    "                for t_idx,table in enumerate(tables):\n",
    "                    df = pd.DataFrame(table)\n",
    "\n",
    "                    all_tables.append({\n",
    "                        \"filing_id\":filing['id'],\n",
    "                        \"company\":filing['company'],\n",
    "                        \"year\":filing['year'],\n",
    "                        \"source\":source_name,\n",
    "                        \"page\":page_idx,\n",
    "                        \"table_id\":f\"{source_name}_p{page_idx}_t{t_idx}\",\n",
    "                        \"df\": df\n",
    "                    })\n",
    "    return all_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a6f4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = extract_tables_from_filings(FILINGS)\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a1fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[11]['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdede63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Products</td>\n",
       "      <td>$</td>\n",
       "      <td>108,803</td>\n",
       "      <td></td>\n",
       "      <td>$ 114,728</td>\n",
       "      <td>$</td>\n",
       "      <td>105,126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Services</td>\n",
       "      <td>60,345</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>56,054</td>\n",
       "      <td>47,710</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total gross margin</td>\n",
       "      <td>$</td>\n",
       "      <td>169,148</td>\n",
       "      <td></td>\n",
       "      <td>$ 170,782</td>\n",
       "      <td>$</td>\n",
       "      <td>152,836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1        2 3          4       5        6\n",
       "0            Products       $  108,803    $ 114,728       $  105,126\n",
       "1            Services  60,345     None       56,054  47,710     None\n",
       "2  Total gross margin       $  169,148    $ 170,782       $  152,836"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[10]['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10529528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products | 36.5 | % | 36.3 | % | 35.3 | %\n",
      "Services | 70.8 | % | 71.7 | % | 69.7 | %\n",
      "Total gross margin percentage | 44.1 | % | 43.3 | % | 41.8 | %\n"
     ]
    }
   ],
   "source": [
    "def table_to_text(df):\n",
    "    df = df.fillna(\"\").astype(str)\n",
    "    lines = []\n",
    "    for row in df.values.tolist():\n",
    "        row = [cell.strip() for cell in row if cell.strip()]\n",
    "        lines.append(\" | \".join(row))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "table_text = table_to_text(tables[11][\"df\"])\n",
    "print(table_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87a899",
   "metadata": {},
   "source": [
    "visit agian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438e3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_year_headers(page_text, max_years=6):\n",
    "    \"\"\"Find unique years like 2023, 2022, 2021 in the page text (keeps order).\"\"\"\n",
    "    years = re.findall(r\"\\b(20\\d{2})\\b\", page_text)\n",
    "    seen = []\n",
    "    for y in years:\n",
    "        if y not in seen:\n",
    "            seen.append(y)\n",
    "    return seen[:max_years]\n",
    "\n",
    "def attach_years_if_possible(df, years):\n",
    "    \"\"\"\n",
    "    If we have years and the row looks like it has 3 numeric values,\n",
    "    produce 'Label | 2023: x | 2022: y | 2021: z' style lines.\n",
    "    Otherwise fallback to plain table_to_text().\n",
    "    \"\"\"\n",
    "    df = df.fillna(\"\").astype(str)\n",
    "\n",
    "    # If no years, just return normal table text\n",
    "    if not years:\n",
    "        return table_to_text(df)\n",
    "\n",
    "    lines = []\n",
    "    for row in df.values.tolist():\n",
    "        # keep non-empty cells\n",
    "        row = [c.strip() for c in row if str(c).strip()]\n",
    "\n",
    "        if not row:\n",
    "            continue\n",
    "\n",
    "        label = row[0]\n",
    "\n",
    "        # Remove \"$\" and \"%\" tokens from values so we can pair cleanly with years\n",
    "        vals = [c for c in row[1:] if c not in {\"$\", \"%\"}]\n",
    "\n",
    "        # If values count matches number of years, pair them\n",
    "        if len(vals) >= len(years) and len(years) >= 2:\n",
    "            pairs = [f\"{y}: {vals[i]}\" for i, y in enumerate(years)]\n",
    "            lines.append(label + \" | \" + \" | \".join(pairs))\n",
    "        else:\n",
    "            # fallback: keep original row\n",
    "            lines.append(\" | \".join(row))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_table_docs(tables, pages):\n",
    "    \"\"\"\n",
    "    tables: output of extract_tables_from_filings\n",
    "    pages:  output of Step 1 (page-by-page text extraction)\n",
    "            IMPORTANT: pages must contain dicts with keys: company, year, page, text\n",
    "    returns: list of table documents (embedding-ready later)\n",
    "    \"\"\"\n",
    "    # Create a quick lookup for page text by (company, year, page)\n",
    "    page_text_lookup = {(p[\"company\"], p[\"year\"], p[\"page\"]): p[\"text\"] for p in pages}\n",
    "\n",
    "    table_docs = []\n",
    "    for t in tables:\n",
    "        page_text = page_text_lookup.get((t[\"company\"], t[\"year\"], t[\"page\"]), \"\")\n",
    "        years = extract_year_headers(page_text)\n",
    "\n",
    "        table_body = attach_years_if_possible(t[\"df\"], years)\n",
    "\n",
    "        table_docs.append({\n",
    "            \"id\": t[\"table_id\"],\n",
    "            \"type\": \"table\",\n",
    "            \"filing_id\": t[\"filing_id\"],\n",
    "            \"company\": t[\"company\"],\n",
    "            \"year\": t[\"year\"],\n",
    "            \"source\": t[\"source\"],\n",
    "            \"page\": t[\"page\"],\n",
    "            \"years_detected\": years,  # helpful for debugging\n",
    "            \"text\": (\n",
    "                f\"[TABLE]\\n\"\n",
    "                f\"Company: {t['company']}\\n\"\n",
    "                f\"FilingYear: {t['year']}\\n\"\n",
    "                f\"Source: {t['source']}\\n\"\n",
    "                f\"Page: {t['page']}\\n\"\n",
    "                f\"Years: {', '.join(years) if years else 'N/A'}\\n\\n\"\n",
    "                f\"{table_body}\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    return table_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54dbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total table docs: 151\n",
      "\n",
      "--- Sample Table Doc ---\n",
      "ID: NOV_2023.pdf_p26_t0\n",
      "Meta: Apple 2023 page 26\n",
      "[TABLE]\n",
      "Company: Apple\n",
      "FilingYear: 2023\n",
      "Source: NOV_2023.pdf\n",
      "Page: 26\n",
      "Years: 2023, 2022, 2021\n",
      "\n",
      "Products | 2023: 108,803 | 2022: $ 114,728 | 2021: 105,126\n",
      "Services | 2023: 60,345 | 2022: 56,054 | 2021: 47,710\n",
      "Total gross margin | 2023: 169,148 | 2022: $ 170,782 | 2021: 152,836\n"
     ]
    }
   ],
   "source": [
    "table_docs = build_table_docs(tables, pages)\n",
    "\n",
    "print(\"Total table docs:\", len(table_docs))\n",
    "\n",
    "# preview one\n",
    "print(\"\\n--- Sample Table Doc ---\")\n",
    "print(\"ID:\", table_docs[10][\"id\"])\n",
    "print(\"Meta:\", table_docs[10][\"company\"], table_docs[10][\"year\"], \"page\", table_docs[10][\"page\"])\n",
    "print(table_docs[10][\"text\"][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d72588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross page overlap\n",
    "\n",
    "def add_cross_page_overlap(pages, tail_chars=400):\n",
    "    \"\"\"\n",
    "    For each page, prepend last N chars from previous page text.\n",
    "    Keeps metadata the same (source/page).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_tail = \"\"\n",
    "    for p in pages:\n",
    "        merged_text = (prev_tail + \" \" + p[\"text\"]).strip()\n",
    "        prev_tail = p[\"text\"][-tail_chars:] if p[\"text\"] else \"\"\n",
    "        out.append({**p, \"text\": merged_text})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c46191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk page text\n",
    "def chunk_text(text, chunk_size=1200, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9fc5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chreate text chunks from pages\n",
    "def build_text_chunks(pages, chunk_size=1200, overlap=200, tail_chars=400):\n",
    "    pages_merged = add_cross_page_overlap(pages, tail_chars=tail_chars)\n",
    "\n",
    "    text_chunks = []\n",
    "    for p in pages_merged:\n",
    "        if not p[\"text\"]:\n",
    "            continue\n",
    "\n",
    "        splits = chunk_text(p[\"text\"], chunk_size=chunk_size, overlap=overlap)\n",
    "        for i, s in enumerate(splits):\n",
    "            text_chunks.append({\n",
    "                \"id\": f\"{p['source']}_p{p['page']}_text_c{i}\",\n",
    "                \"type\": \"text\",\n",
    "                \"company\": p[\"company\"],\n",
    "                \"year\": p[\"year\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"page\": p[\"page\"],\n",
    "                \"text\": s\n",
    "            })\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8c12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table chunks\n",
    "\n",
    "def build_table_chunks(table_docs, max_chars=3500):\n",
    "    table_chunks = []\n",
    "    for t in table_docs:\n",
    "        txt = t[\"text\"].strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # If table text is short enough, keep as one chunk\n",
    "        if len(txt) <= max_chars:\n",
    "            table_chunks.append({\n",
    "                \"id\": t[\"id\"],\n",
    "                \"type\": \"table\",\n",
    "                \"company\": t[\"company\"],\n",
    "                \"year\": t[\"year\"],\n",
    "                \"source\": t[\"source\"],\n",
    "                \"page\": t[\"page\"],\n",
    "                \"text\": txt\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Else split table by lines into groups\n",
    "        lines = txt.splitlines()\n",
    "        buf, part = [], 0\n",
    "        cur_len = 0\n",
    "        for line in lines:\n",
    "            if cur_len + len(line) + 1 > max_chars and buf:\n",
    "                table_chunks.append({\n",
    "                    \"id\": f\"{t['id']}_part{part}\",\n",
    "                    \"type\": \"table\",\n",
    "                    \"company\": t[\"company\"],\n",
    "                    \"year\": t[\"year\"],\n",
    "                    \"source\": t[\"source\"],\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"text\": \"\\n\".join(buf)\n",
    "                })\n",
    "                buf, cur_len = [], 0\n",
    "                part += 1\n",
    "            buf.append(line)\n",
    "            cur_len += len(line) + 1\n",
    "\n",
    "        if buf:\n",
    "            table_chunks.append({\n",
    "                \"id\": f\"{t['id']}_part{part}\",\n",
    "                \"type\": \"table\",\n",
    "                \"company\": t[\"company\"],\n",
    "                \"year\": t[\"year\"],\n",
    "                \"source\": t[\"source\"],\n",
    "                \"page\": t[\"page\"],\n",
    "                \"text\": \"\\n\".join(buf)\n",
    "            })\n",
    "\n",
    "    return table_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5147418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunks: 1345\n",
      "Table chunks: 152\n",
      "Total chunks: 1497\n",
      "\n",
      "Sample text chunk:\n",
      " NOV_2023.pdf_p1_text_c0 | page 1\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington DC 20549\n",
      "FORM 10K\n",
      "Mark One\n",
      " ANNUAL REPORT PURSUANT TO SECTION 13 OR 15d OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September 30 2023\n",
      "or\n",
      " TRANSITION REPORT PURSUANT TO SECTION 13 OR 15d OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from              to             \n",
      "Commission File Number 00136743\n",
      "Ap\n",
      "\n",
      "Sample table chunk:\n",
      " NOV_2023.pdf_p3_t0 | page 3\n",
      "[TABLE]\n",
      "Company: Apple\n",
      "FilingYear: 2023\n",
      "Source: NOV_2023.pdf\n",
      "Page: 3\n",
      "Years: 2023\n",
      "\n",
      "Item 1. | Business | 1\n",
      "Item 1A. | Risk Factors | 5\n",
      "Item 1B. | Unresolved Staff Comments | 16\n",
      "Item 1C. | Cybersecurity | 16\n",
      "Item 2. | Properties | 17\n",
      "Item 3. | Legal Proceedings | 17\n",
      "Item 4. | Mine Safety Disclosures | 17\n"
     ]
    }
   ],
   "source": [
    "# final chunks\n",
    "\n",
    "text_chunks = build_text_chunks(pages, chunk_size=1200, overlap=200, tail_chars=400)\n",
    "table_chunks = build_table_chunks(table_docs, max_chars=3500)\n",
    "\n",
    "chunks = text_chunks + table_chunks\n",
    "\n",
    "print(\"Text chunks:\", len(text_chunks))\n",
    "print(\"Table chunks:\", len(table_chunks))\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "print(\"\\nSample text chunk:\\n\", chunks[0][\"id\"], \"| page\", chunks[0][\"page\"])\n",
    "print(chunks[0][\"text\"][:400])\n",
    "\n",
    "print(\"\\nSample table chunk:\\n\", table_chunks[0][\"id\"], \"| page\", table_chunks[0][\"page\"])\n",
    "print(table_chunks[0][\"text\"][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed632456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # folder created locally\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    return s[:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b3ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_LOCAL_MODEL_CACHE = {}\n",
    "\n",
    "def _get_local_model(model_name: str):\n",
    "    if model_name in _LOCAL_MODEL_CACHE:\n",
    "        return _LOCAL_MODEL_CACHE[model_name]\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    m = SentenceTransformer(model_name, device=device)\n",
    "    _LOCAL_MODEL_CACHE[model_name] = m\n",
    "    return m\n",
    "\n",
    "def _is_e5(model_name: str) -> bool:\n",
    "    return \"e5\" in model_name.lower()\n",
    "\n",
    "def embed_texts(texts, cfg, *, mode=\"doc\"):\n",
    "    provider = cfg[\"provider\"].lower()\n",
    "\n",
    "    # ---- Local (GPU) ----\n",
    "    if provider == \"local\":\n",
    "        model = _get_local_model(cfg[\"model_name\"])\n",
    "        if _is_e5(cfg[\"model_name\"]):\n",
    "            prefix = \"query: \" if mode == \"query\" else \"doc: \"\n",
    "            texts = [prefix + t for t in texts]\n",
    "        vecs = model.encode(texts, batch_size=64, normalize_embeddings=True, show_progress_bar=False)\n",
    "        return vecs.tolist()\n",
    "\n",
    "\n",
    "# ---- Google (Gemini) ----\n",
    "    if provider == \"google\":\n",
    "        from google import genai\n",
    "        api_key = os.getenv(cfg[\"api_key_env\"])\n",
    "        if not api_key:\n",
    "            raise ValueError(f\"Missing env var: {cfg['api_key_env']}\")\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            r = client.models.embed_content(model=cfg[\"model_name\"], contents=t)\n",
    "            out.append(r.embeddings[0].values)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(f\"Unknown provider: {cfg['provider']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- OpenAI ----\n",
    "    '''\n",
    "    if provider == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        api_key = os.getenv(cfg[\"api_key_env\"])\n",
    "        if not api_key:\n",
    "            raise ValueError(f\"Missing env var: {cfg['api_key_env']}\")\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        resp = client.embeddings.create(model=cfg[\"model_name\"], input=texts)\n",
    "        return [d.embedding for d in resp.data]'''\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbc92865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_chunks_to_chroma(chunks, cfg, batch_size=64):\n",
    "    # Collection name encodes provider+model+dim so no mismatch\n",
    "    collection_name = safe_name(\n",
    "        f\"{cfg['provider']}_{cfg['model_name']}_{cfg['dimensions']}\"\n",
    "    )\n",
    "\n",
    "    # Create or get collection (use cosine space)\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    # Add in batches\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        texts = [c[\"text\"] for c in batch]\n",
    "        ids = [c[\"id\"] for c in batch]\n",
    "\n",
    "        # store only JSON-serializable metadata\n",
    "        metadatas = [{\n",
    "            \"type\": c[\"type\"],\n",
    "            \"company\": c[\"company\"],\n",
    "            \"year\": str(c[\"year\"]),\n",
    "            \"source\": c[\"source\"],\n",
    "            \"page\": int(c[\"page\"]),\n",
    "        } for c in batch]\n",
    "\n",
    "        embs = embed_texts(texts, cfg, mode=\"doc\")\n",
    "\n",
    "        # sanity check: dimension match\n",
    "        got_dim = len(embs[0])\n",
    "        if got_dim != cfg[\"dimensions\"]:\n",
    "            raise ValueError(\n",
    "                f\"Dim mismatch for {cfg['provider']} {cfg['model_name']}: \"\n",
    "                f\"expected {cfg['dimensions']}, got {got_dim}\"\n",
    "            )\n",
    "\n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas,\n",
    "            embeddings=embs\n",
    "        )\n",
    "\n",
    "    return collection_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea209f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed: google_text_embedding_004_768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed: local_all_minilm_l6_v2_384\n",
      "Indexed: local_intfloat_e5_large_v2_1024\n",
      "\n",
      "All collections created:\n",
      " - google_text_embedding_004_768\n",
      " - local_all_minilm_l6_v2_384\n",
      " - local_intfloat_e5_large_v2_1024\n"
     ]
    }
   ],
   "source": [
    "created = []\n",
    "for cfg in embedding_models:\n",
    "    name = upsert_chunks_to_chroma(chunks, cfg, batch_size=64)\n",
    "    created.append(name)\n",
    "    print(\"Indexed:\", name)\n",
    "\n",
    "print(\"\\nAll collections created:\")\n",
    "for n in created:\n",
    "    print(\" -\", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a974546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_for_model(cfg):\n",
    "    collection_name = safe_name(f\"{cfg['provider']}_{cfg['model_name']}_{cfg['dimensions']}\")\n",
    "    return chroma_client.get_collection(name=collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240cf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_topk_with_cfg(collection, query, cfg, k=8, where=None):\n",
    "    q_emb = embed_texts([query], cfg, mode=\"query\")[0]  # correct dims\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        where=where,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    hits = []\n",
    "    for _id, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        hits.append({\"id\": _id, \"text\": doc, \"meta\": meta, \"distance\": dist})\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa670144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_text_chunk_id(chunk_id):\n",
    "    \"\"\"\n",
    "    Extract (source, page, chunk_index) from ids like:\n",
    "    Tesla_2023.pdf_p32_text_c3\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_p(\\d+)_text_c(\\d+)$\", chunk_id)\n",
    "    if not m:\n",
    "        return None\n",
    "    return (m.group(1), int(m.group(2)), int(m.group(3)))\n",
    "\n",
    "def expand_neighbors(collection, hits, window=1, max_neighbors=30):\n",
    "    \"\"\"\n",
    "    For each hit, include +/- window neighboring chunks if they exist.\n",
    "    Only applies to text chunks with _text_cX ids.\n",
    "    \"\"\"\n",
    "    wanted_ids = set(h[\"id\"] for h in hits)\n",
    "\n",
    "    for h in hits:\n",
    "        parsed = parse_text_chunk_id(h[\"id\"])\n",
    "        if not parsed:\n",
    "            continue\n",
    "        source, page, cidx = parsed\n",
    "        for j in range(cidx - window, cidx + window + 1):\n",
    "            if j < 0:\n",
    "                continue\n",
    "            nid = f\"{source}_p{page}_text_c{j}\"\n",
    "            wanted_ids.add(nid)\n",
    "            if len(wanted_ids) >= max_neighbors + len(hits):\n",
    "                break\n",
    "\n",
    "    # fetch neighbor docs by ids (get)\n",
    "    got = collection.get(ids=list(wanted_ids), include=[\"documents\", \"metadatas\"])\n",
    "    out = []\n",
    "    for _id, doc, meta in zip(got[\"ids\"], got[\"documents\"], got[\"metadatas\"]):\n",
    "        out.append({\"id\": _id, \"text\": doc, \"meta\": meta})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39d5a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_and_sort(hits):\n",
    "    seen = {}\n",
    "    for h in hits:\n",
    "        seen[h[\"id\"]] = h\n",
    "\n",
    "    final = list(seen.values())\n",
    "    final.sort(key=lambda x: (\n",
    "        x[\"meta\"].get(\"source\", \"\"),\n",
    "        int(x[\"meta\"].get(\"page\", 0)),\n",
    "        x[\"meta\"].get(\"type\", \"\"),\n",
    "        x[\"id\"]\n",
    "    ))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c4167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_pack(chunks):\n",
    "    \"\"\"\n",
    "    Returns a single context string + a citation list.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    citations = []\n",
    "    for c in chunks:\n",
    "        src = c[\"meta\"].get(\"source\")\n",
    "        page = c[\"meta\"].get(\"page\")\n",
    "        ctype = c[\"meta\"].get(\"type\")\n",
    "        blocks.append(\n",
    "            f\"[{ctype.upper()}] {src} (page {page})\\n{c['text']}\".strip()\n",
    "        )\n",
    "        citations.append({\"id\": c[\"id\"], \"source\": src, \"page\": page, \"type\": ctype})\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(blocks)\n",
    "    return context, citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a75798d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncfg = embedding_models[1]  # Google text-embedding-004 (for example)\\ncollection = get_collection_for_model(cfg)\\n\\nquestion = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\\nhits = retrieve_topk(collection, question, k=8)\\n\\n# expand neighbors (helps continuity)\\nexpanded = expand_neighbors(collection, hits, window=1)\\n\\nfinal_chunks = dedupe_and_sort(hits + expanded)\\ncontext, citations = build_context_pack(final_chunks)\\n\\nprint(\"Retrieved chunks:\", len(final_chunks))\\nprint(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\\nprint(context[:1200])\\n\\nprint(\"\\n--- CITATIONS ---\")\\nprint(citations[:10])\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cfg = embedding_models[1]  # Google text-embedding-004 (for example)\n",
    "collection = get_collection_for_model(cfg)\n",
    "\n",
    "question = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\n",
    "hits = retrieve_topk(collection, question, k=8)\n",
    "\n",
    "# expand neighbors (helps continuity)\n",
    "expanded = expand_neighbors(collection, hits, window=1)\n",
    "\n",
    "final_chunks = dedupe_and_sort(hits + expanded)\n",
    "context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "print(\"Retrieved chunks:\", len(final_chunks))\n",
    "print(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\n",
    "print(context[:1200])\n",
    "\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(citations[:10])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8990139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'provider': 'Local', 'model_name': 'all-MiniLM-L6-v2', 'api_key_env': None, 'dimensions': 384}\n",
      "Retrieved chunks: 11\n",
      "\n",
      "--- CONTEXT (first 1200 chars) ---\n",
      "\n",
      "[TEXT] NOV_2023.pdf (page 24)\n",
      "Company repurchased 766 billion of its common stock and paid dividends and dividend equivalents of 150 billion\n",
      "Macroeconomic Conditions\n",
      "Macroeconomic conditions including inflation changes in interest rates and currency fluctuations have directly and indirectly impacted and could in the future\n",
      "materially impact the Companys results of operations and financial condition\n",
      "Apple Inc  2023 Form 10K  20 Segment Operating Performance\n",
      "The following table shows net sales by reportable segment for 2023 2022 and 2021 dollars in millions\n",
      "2023\n",
      "Change\n",
      "2022\n",
      "Change\n",
      "2021\n",
      "Net sales by reportable segment\n",
      "Americas\n",
      "\n",
      "162560 \n",
      "4\n",
      "\n",
      "169658 \n",
      "11 \n",
      "\n",
      "153306 \n",
      "Europe\n",
      "94294 \n",
      "1\n",
      "95118 \n",
      "7 \n",
      "89307 \n",
      "Greater China\n",
      "72559 \n",
      "2\n",
      "74200 \n",
      "9 \n",
      "68366 \n",
      "Japan\n",
      "24257 \n",
      "7\n",
      "25977 \n",
      "9\n",
      "28482 \n",
      "Rest of Asia Pacific\n",
      "29615 \n",
      "1 \n",
      "29375 \n",
      "11 \n",
      "26356 \n",
      "Total net sales\n",
      "\n",
      "383285 \n",
      "3\n",
      "\n",
      "394328 \n",
      "8 \n",
      "\n",
      "365817 \n",
      "Americas\n",
      "Americas net sales decreased 4 or 71 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac partially offset by higher net sales of\n",
      "Services\n",
      "Europe\n",
      "Europe net sales decreased 1 or 824 million during 2023 compared to 2022 The weakness in foreign currencies relative to the US dollar accounted for\n",
      "more t\n",
      "\n",
      "--- CITATIONS ---\n",
      "[{'id': 'NOV_2023.pdf_p24_text_c0', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p24_text_c1', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p24_text_c2', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_t0', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'table'}, {'id': 'NOV_2023.pdf_p26_t1', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'table'}, {'id': 'NOV_2023.pdf_p26_text_c0', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_text_c1', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_text_c2', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p38_text_c0', 'source': 'NOV_2023.pdf', 'page': 38, 'type': 'text'}, {'id': 'NOV_2023.pdf_p38_text_c1', 'source': 'NOV_2023.pdf', 'page': 38, 'type': 'text'}]\n"
     ]
    }
   ],
   "source": [
    "cfg = embedding_models[1]\n",
    "print(cfg)  # Google text-embedding-004\n",
    "collection = get_collection_for_model(cfg)\n",
    "\n",
    "question = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\n",
    "hits = retrieve_topk_with_cfg(collection, question, cfg, k=8, where={\"company\":\"Apple\"})  \n",
    "\n",
    "expanded = expand_neighbors(collection, hits, window=1)\n",
    "\n",
    "final_chunks = dedupe_and_sort(hits + expanded)\n",
    "context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "print(\"Retrieved chunks:\", len(final_chunks))\n",
    "print(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\n",
    "print(context[:1200])\n",
    "\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(citations[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bfce2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are a financial filing assistant.\n",
    "\n",
    "RULES:\n",
    "- Use ONLY the provided context.\n",
    "- If the answer is not in the context, say: \"I don't have enough information in the provided context.\"\n",
    "- For every numeric claim, include a citation in this format: (Source: <file>, Page: <page>).\n",
    "- Keep the answer concise and structured.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a776f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "def generate_answer_gemini(question, context, model=\"gemini-2.0-flash\"):\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing GEMINI_API_KEY in environment variables\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    prompt = build_rag_prompt(question, context)\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return resp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35ef04b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 24.643963572s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m, in \u001b[0;36mgenerate_answer_gemini\u001b[1;34m(question, context, model)\u001b[0m\n\u001b[0;32m      9\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_rag_prompt(question, context)\n\u001b[1;32m---> 12\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\models.py:5203\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5201\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5202\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5203\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5204\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[0;32m   5205\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5207\u001b[0m   function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n\u001b[0;32m   5208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\models.py:3985\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   3982\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   3983\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 3985\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3986\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   3987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   3990\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3991\u001b[0m ):\n\u001b[0;32m   3992\u001b[0m   return_value \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse(sdk_http_response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1380\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1383\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SdkHttpResponse:\n\u001b[0;32m   1385\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m   1386\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m   1387\u001b[0m   )\n\u001b[1;32m-> 1388\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m   response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1390\u001b[0m       response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1391\u001b[0m   )\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\_api_client.py:1224\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[0;32m   1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[1;32m-> 1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:420\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    418\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:187\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m   1195\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   1196\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1199\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m   1200\u001b[0m   )\n\u001b[1;32m-> 1201\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m   1203\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m   1204\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\errors.py:121\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m   response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mbody_segments[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\genai\\errors.py:146\u001b[0m, in \u001b[0;36mAPIError.raise_error\u001b[1;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[1;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 24.643963572s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}"
     ]
    }
   ],
   "source": [
    "answer = generate_answer_gemini(question, context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9f8a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RUNS = [\n",
    "  {\"name\":\"google_768\", \"cfg\": embedding_models[0], \"collection\":\"google_text_embedding_004_768\"},\n",
    "  {\"name\":\"minilm_384\", \"cfg\": embedding_models[1], \"collection\":\"local_all_minilm_l6_v2_384\"},\n",
    "  {\"name\":\"e5_1024\",    \"cfg\": embedding_models[2], \"collection\":\"local_intfloat_e5_large_v2_1024\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14edc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "  {\"id\":\"q1\", \"company\":\"Apple\", \"question\":\"What was total gross margin in 2023 vs 2022 vs 2021?\"},\n",
    "  {\"id\":\"q2\", \"company\":\"Apple\", \"question\":\"What are the main risk factors mentioned about supply chain?\"},\n",
    "  {\"id\":\"q3\", \"company\":\"Tesla\", \"question\":\"What does Tesla say about revenue recognition?\"},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb26c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_rag_json_prompt(question, context):\n",
    "    schema = {\n",
    "      \"question\": \"string\",\n",
    "      \"answer\": \"string\",\n",
    "      \"citations\": [{\"source\":\"string\", \"page\": \"number\", \"type\":\"text|table\"}],\n",
    "      \"support_level\": \"high|medium|low\",\n",
    "      \"missing_info\": \"string|null\"\n",
    "    }\n",
    "    return f\"\"\"\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the provided context.\n",
    "- If not enough info, set support_level=\"low\" and missing_info with what is missing.\n",
    "- Every numeric claim must have citations with correct source+page.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad1e594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_gemini_json(question, context, model=\"gemini-2.5-pro\"):\n",
    "  \n",
    "    import os\n",
    "    from google import genai\n",
    "\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing GEMINI_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    prompt = build_rag_json_prompt(question, context)\n",
    "    resp = client.models.generate_content(model=model, contents=prompt)\n",
    "    txt = resp.text.strip()\n",
    "\n",
    "    # best effort JSON parse\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": txt,\n",
    "            \"citations\": [],\n",
    "            \"support_level\": \"low\",\n",
    "            \"missing_info\": \"Model did not return valid JSON\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15d5ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def p50(xs): return float(np.percentile(xs, 50)) if xs else None\n",
    "def p90(xs): return float(np.percentile(xs, 90)) if xs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdcb6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def estimate_citation_coverage(answer_text: str):\n",
    "    # counts \"(Source:\" occurrences as citations\n",
    "    cites = len(re.findall(r\"\\(Source:\", answer_text))\n",
    "    # rough sentence count\n",
    "    sents = max(1, len(re.findall(r\"[.!?]\\s+\", answer_text)) + 1)\n",
    "    return min(1.0, cites / sents)\n",
    "\n",
    "def support_level_to_score(level: str):\n",
    "    level = (level or \"\").lower()\n",
    "    return {\"high\": 1.0, \"medium\": 0.6, \"low\": 0.2}.get(level, 0.2)\n",
    "\n",
    "def retrieval_score_from_hits(hits):\n",
    "    # use best distance only; if missing, fallback\n",
    "    dists = [h.get(\"distance\") for h in hits if h.get(\"distance\") is not None]\n",
    "    if not dists:\n",
    "        return 0.5\n",
    "    best = min(dists)\n",
    "    # turn distance into a bounded score (tunable)\n",
    "    return float(1 / (1 + best))\n",
    "\n",
    "def confidence_score(hits, gen_json):\n",
    "    ans = gen_json.get(\"answer\", \"\")\n",
    "    cov = estimate_citation_coverage(ans)\n",
    "    sup = support_level_to_score(gen_json.get(\"support_level\"))\n",
    "\n",
    "    rscore = retrieval_score_from_hits(hits)\n",
    "\n",
    "    # weighted composite\n",
    "    return round(0.45*rscore + 0.35*cov + 0.20*sup, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e4917a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RagChat:\n",
    "    def __init__(self, max_turns=6):\n",
    "        self.history = deque(maxlen=max_turns)\n",
    "\n",
    "    def add_turn(self, q, a):\n",
    "        self.history.append({\"question\": q, \"answer\": a})\n",
    "\n",
    "    def history_block(self):\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        items = []\n",
    "        for t in self.history:\n",
    "            items.append(f\"User: {t['question']}\\nAssistant: {t['answer']}\")\n",
    "        return \"\\n\\n\".join(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d46db50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_json_prompt_with_history(question, context, history_text):\n",
    "    base = build_rag_json_prompt(question, context)\n",
    "    if history_text.strip():\n",
    "        return f\"HISTORY:\\n{history_text}\\n\\n{base}\"\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27426a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_question(model_run, question_obj, *, k=8):\n",
    "    cfg = model_run[\"cfg\"]\n",
    "    collection = chroma_client.get_collection(model_run[\"collection\"])\n",
    "\n",
    "    question = question_obj[\"question\"]\n",
    "    where = {\"company\": question_obj[\"company\"]} if \"company\" in question_obj else None\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    hits = retrieve_topk_with_cfg(collection, question, cfg, k=k, where=where)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    expanded = expand_neighbors(collection, hits, window=1)\n",
    "    final_chunks = dedupe_and_sort(hits + expanded)\n",
    "    context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    gen = generate_answer_gemini_json(question, context)\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    # compute confidence\n",
    "    conf = confidence_score(hits, gen)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_run[\"name\"],\n",
    "        \"question_id\": question_obj.get(\"id\"),\n",
    "        \"company\": question_obj.get(\"company\"),\n",
    "        \"question\": question,\n",
    "        \"retrieval_ms\": round((t1 - t0) * 1000, 2),\n",
    "        \"generation_ms\": round((t3 - t2) * 1000, 2),\n",
    "        \"end_to_end_ms\": round((t3 - t0) * 1000, 2),\n",
    "        \"confidence\": conf,\n",
    "        \"response_json\": gen,\n",
    "        \"top_sources\": list({(c[\"source\"], c[\"page\"]) for c in citations})[:6],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38c11dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9] google_768 | What was total gross margin in 2023 vs 2022 vs 202...\n",
      "  ⚠ Skipped after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\nPlease retry in 27.645336177s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}\n",
      "[2/9] minilm_384 | What was total gross margin in 2023 vs 2022 vs 202...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 56\u001b[0m\n\u001b[0;32m     44\u001b[0m         summary[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(rows),\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(conf)), \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me2e_p90_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m: p90(e2e),\n\u001b[0;32m     53\u001b[0m         }\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, summary\n\u001b[1;32m---> 56\u001b[0m results, summary \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_RUNS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEVAL_QUESTIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay_between\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[57], line 25\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(models, questions, delay_between)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\__init__.py:487\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    486\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32mc:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from google.genai.errors import ClientError\n",
    "\n",
    "@retry(\n",
    "    retry=retry_if_exception_type(ClientError),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    reraise=True\n",
    ")\n",
    "def run_one_with_retry(m, q, k=8):\n",
    "    return run_one_question(m, q, k=k)\n",
    "\n",
    "def benchmark(models, questions, delay_between=2.0):\n",
    "    results = []\n",
    "    total = len(questions) * len(models)\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        for j, m in enumerate(models):\n",
    "            idx = i * len(models) + j + 1\n",
    "            print(f\"[{idx}/{total}] {m['name']} | {q['question'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                r = run_one_with_retry(m, q, k=8)\n",
    "                results.append(r)\n",
    "            except ClientError as e:\n",
    "                print(f\"  ⚠ Skipped after retries: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # throttle to stay under RPM limits\n",
    "            time.sleep(delay_between)\n",
    "\n",
    "    # summarize latency percentiles by model\n",
    "    summary = {}\n",
    "    for m in models:\n",
    "        name = m[\"name\"]\n",
    "        rows = [r for r in results if r[\"model\"] == name]\n",
    "        retr = [r[\"retrieval_ms\"] for r in rows]\n",
    "        gen  = [r[\"generation_ms\"] for r in rows]\n",
    "        e2e  = [r[\"end_to_end_ms\"] for r in rows]\n",
    "        conf = [r[\"confidence\"] for r in rows]\n",
    "\n",
    "        summary[name] = {\n",
    "            \"n\": len(rows),\n",
    "            \"confidence_avg\": round(float(np.mean(conf)), 4) if conf else None,\n",
    "            \"retrieval_p50_ms\": p50(retr),\n",
    "            \"retrieval_p90_ms\": p90(retr),\n",
    "            \"generation_p50_ms\": p50(gen),\n",
    "            \"generation_p90_ms\": p90(gen),\n",
    "            \"e2e_p50_ms\": p50(e2e),\n",
    "            \"e2e_p90_ms\": p90(e2e),\n",
    "        }\n",
    "    return results, summary\n",
    "\n",
    "results, summary = benchmark(MODEL_RUNS, EVAL_QUESTIONS, delay_between=2.0)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173358bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
