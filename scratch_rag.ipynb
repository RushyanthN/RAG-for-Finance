{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a253b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport shutil\\n\\nif os.path.exists(\"./chroma_db\"):\\n    shutil.rmtree(\"./chroma_db\")\\n    print(\"ChromaDB deleted\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    shutil.rmtree(\"./chroma_db\")\n",
    "    print(\"ChromaDB deleted\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9b8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"API keys loaded\" if LLAMA_CLOUD_API_KEY and GEMINI_API_KEY else \"✗ Missing API keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d39366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILINGS= [\n",
    "    {\"id\":\"1\",\"company\":\"Apple\",\"year\":\"2023\",\"path\": r\"C:\\Users\\rushy\\Downloads\\FINBOT\\GenAI_FInBot\\NOV_2023.pdf\"},\n",
    "    {\"id\":\"2\",\"company\":\"Tesla\",\"year\":\"2023\",\"path\": r\"C:\\Users\\rushy\\Downloads\\FINBOT\\GenAI_FInBot\\Tesla_2023.pdf\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a70e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = [\n",
    "    #{\"provider\":\"OpenAI\",\"model_name\":\"text-embedding-3-small\",\"api_key_env\":\"OPENAI_API_KEY\",\"dimensions\":1536},\n",
    "    {\"provider\":\"Google\",\"model_name\":\"text-embedding-004\",\"api_key_env\":\"GEMINI_API_KEY\",\"dimensions\":768},\n",
    "    {\"provider\":\"Local\",\"model_name\":\"all-MiniLM-L6-v2\",\"api_key_env\":None,\"dimensions\":384},\n",
    "    {\"provider\":\"Local\",\"model_name\":\"intfloat/e5-large-v2\",\"api_key_env\":None,\"dimensions\":1024},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab735a",
   "metadata": {},
   "source": [
    "Need to check\n",
    "1. Latency p90\n",
    "2. Recall p50\n",
    "3. Semantic Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe0b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65267b9",
   "metadata": {},
   "source": [
    "Pipleline Skeleton\n",
    "1. pages = extract_pages_text(pdf_paths)\n",
    "2. tables = extract_tables(pdf_paths)\n",
    "3. docs = build_docs(pages,tables)\n",
    "4. chunks = chuk_docs(docs)\n",
    "5. index(chuks,embedding_model_cfg)\n",
    "6. retrieve(query) -> context\n",
    "7. generate(context,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbcdaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(text:str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).strip()\n",
    "\n",
    "\n",
    "def extract_pages_from_filings(filings):\n",
    "    all_pages = []\n",
    "\n",
    "    for filing in filings:\n",
    "        doc = fitz.open(filing['path'])\n",
    "        source_name = Path(filing['path']).name\n",
    "\n",
    "        for page_idx,page in enumerate(doc,start=1):\n",
    "            text = page.get_text(\"text\")\n",
    "            #print(text)\n",
    "            all_pages.append({\n",
    "                \"filing_id\":filing['id'],\n",
    "                \"company\": filing['company'],\n",
    "                \"year\":filing['year'],\n",
    "                \"source\": source_name,\n",
    "                \"page\":page_idx,\n",
    "                \"text\":clean_text(text),\n",
    "            })\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    return all_pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fa1d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = extract_pages_from_filings(FILINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aec7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_tables_from_filings(filings,*,max_tables_per_page=None):\n",
    "    all_tables = []\n",
    "    for filing in filings:\n",
    "        pdf_path = filing['path']\n",
    "        source_name = Path(pdf_path).name\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_idx,page in enumerate(pdf.pages,start=1):\n",
    "                tables = page.extract_tables()\n",
    "\n",
    "                if not tables:\n",
    "                    continue\n",
    "\n",
    "                if max_tables_per_page is not None:\n",
    "                    tables = tables[:max_tables_per_page]\n",
    "\n",
    "                for t_idx,table in enumerate(tables):\n",
    "                    df = pd.DataFrame(table)\n",
    "\n",
    "                    all_tables.append({\n",
    "                        \"filing_id\":filing['id'],\n",
    "                        \"company\":filing['company'],\n",
    "                        \"year\":filing['year'],\n",
    "                        \"source\":source_name,\n",
    "                        \"page\":page_idx,\n",
    "                        \"table_id\":f\"{source_name}_p{page_idx}_t{t_idx}\",\n",
    "                        \"df\": df\n",
    "                    })\n",
    "    return all_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a6f4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = extract_tables_from_filings(FILINGS)\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a1fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[11]['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdede63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Products</td>\n",
       "      <td>$</td>\n",
       "      <td>108,803</td>\n",
       "      <td></td>\n",
       "      <td>$ 114,728</td>\n",
       "      <td>$</td>\n",
       "      <td>105,126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Services</td>\n",
       "      <td>60,345</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>56,054</td>\n",
       "      <td>47,710</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total gross margin</td>\n",
       "      <td>$</td>\n",
       "      <td>169,148</td>\n",
       "      <td></td>\n",
       "      <td>$ 170,782</td>\n",
       "      <td>$</td>\n",
       "      <td>152,836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1        2 3          4       5        6\n",
       "0            Products       $  108,803    $ 114,728       $  105,126\n",
       "1            Services  60,345     None       56,054  47,710     None\n",
       "2  Total gross margin       $  169,148    $ 170,782       $  152,836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[10]['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10529528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products | 36.5 | % | 36.3 | % | 35.3 | %\n",
      "Services | 70.8 | % | 71.7 | % | 69.7 | %\n",
      "Total gross margin percentage | 44.1 | % | 43.3 | % | 41.8 | %\n"
     ]
    }
   ],
   "source": [
    "def table_to_text(df):\n",
    "    df = df.fillna(\"\").astype(str)\n",
    "    lines = []\n",
    "    for row in df.values.tolist():\n",
    "        row = [cell.strip() for cell in row if cell.strip()]\n",
    "        lines.append(\" | \".join(row))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "table_text = table_to_text(tables[11][\"df\"])\n",
    "print(table_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87a899",
   "metadata": {},
   "source": [
    "visit agian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438e3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_year_headers(page_text, max_years=6):\n",
    "    \"\"\"Find unique years like 2023, 2022, 2021 in the page text (keeps order).\"\"\"\n",
    "    years = re.findall(r\"\\b(20\\d{2})\\b\", page_text)\n",
    "    seen = []\n",
    "    for y in years:\n",
    "        if y not in seen:\n",
    "            seen.append(y)\n",
    "    return seen[:max_years]\n",
    "\n",
    "def attach_years_if_possible(df, years):\n",
    "    \"\"\"\n",
    "    If we have years and the row looks like it has 3 numeric values,\n",
    "    produce 'Label | 2023: x | 2022: y | 2021: z' style lines.\n",
    "    Otherwise fallback to plain table_to_text().\n",
    "    \"\"\"\n",
    "    df = df.fillna(\"\").astype(str)\n",
    "\n",
    "    # If no years, just return normal table text\n",
    "    if not years:\n",
    "        return table_to_text(df)\n",
    "\n",
    "    lines = []\n",
    "    for row in df.values.tolist():\n",
    "        # keep non-empty cells\n",
    "        row = [c.strip() for c in row if str(c).strip()]\n",
    "\n",
    "        if not row:\n",
    "            continue\n",
    "\n",
    "        label = row[0]\n",
    "\n",
    "        # Remove \"$\" and \"%\" tokens from values so we can pair cleanly with years\n",
    "        vals = [c for c in row[1:] if c not in {\"$\", \"%\"}]\n",
    "\n",
    "        # If values count matches number of years, pair them\n",
    "        if len(vals) >= len(years) and len(years) >= 2:\n",
    "            pairs = [f\"{y}: {vals[i]}\" for i, y in enumerate(years)]\n",
    "            lines.append(label + \" | \" + \" | \".join(pairs))\n",
    "        else:\n",
    "            # fallback: keep original row\n",
    "            lines.append(\" | \".join(row))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_table_docs(tables, pages):\n",
    "    \"\"\"\n",
    "    tables: output of extract_tables_from_filings\n",
    "    pages:  output of Step 1 (page-by-page text extraction)\n",
    "            IMPORTANT: pages must contain dicts with keys: company, year, page, text\n",
    "    returns: list of table documents (embedding-ready later)\n",
    "    \"\"\"\n",
    "    # Create a quick lookup for page text by (company, year, page)\n",
    "    page_text_lookup = {(p[\"company\"], p[\"year\"], p[\"page\"]): p[\"text\"] for p in pages}\n",
    "\n",
    "    table_docs = []\n",
    "    for t in tables:\n",
    "        page_text = page_text_lookup.get((t[\"company\"], t[\"year\"], t[\"page\"]), \"\")\n",
    "        years = extract_year_headers(page_text)\n",
    "\n",
    "        table_body = attach_years_if_possible(t[\"df\"], years)\n",
    "\n",
    "        table_docs.append({\n",
    "            \"id\": t[\"table_id\"],\n",
    "            \"type\": \"table\",\n",
    "            \"filing_id\": t[\"filing_id\"],\n",
    "            \"company\": t[\"company\"],\n",
    "            \"year\": t[\"year\"],\n",
    "            \"source\": t[\"source\"],\n",
    "            \"page\": t[\"page\"],\n",
    "            \"years_detected\": years,  # helpful for debugging\n",
    "            \"text\": (\n",
    "                f\"[TABLE]\\n\"\n",
    "                f\"Company: {t['company']}\\n\"\n",
    "                f\"FilingYear: {t['year']}\\n\"\n",
    "                f\"Source: {t['source']}\\n\"\n",
    "                f\"Page: {t['page']}\\n\"\n",
    "                f\"Years: {', '.join(years) if years else 'N/A'}\\n\\n\"\n",
    "                f\"{table_body}\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    return table_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b54dbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total table docs: 151\n",
      "\n",
      "--- Sample Table Doc ---\n",
      "ID: NOV_2023.pdf_p26_t0\n",
      "Meta: Apple 2023 page 26\n",
      "[TABLE]\n",
      "Company: Apple\n",
      "FilingYear: 2023\n",
      "Source: NOV_2023.pdf\n",
      "Page: 26\n",
      "Years: 2023, 2022, 2021\n",
      "\n",
      "Products | 2023: 108,803 | 2022: $ 114,728 | 2021: 105,126\n",
      "Services | 2023: 60,345 | 2022: 56,054 | 2021: 47,710\n",
      "Total gross margin | 2023: 169,148 | 2022: $ 170,782 | 2021: 152,836\n"
     ]
    }
   ],
   "source": [
    "table_docs = build_table_docs(tables, pages)\n",
    "\n",
    "print(\"Total table docs:\", len(table_docs))\n",
    "\n",
    "# preview one\n",
    "print(\"\\n--- Sample Table Doc ---\")\n",
    "print(\"ID:\", table_docs[10][\"id\"])\n",
    "print(\"Meta:\", table_docs[10][\"company\"], table_docs[10][\"year\"], \"page\", table_docs[10][\"page\"])\n",
    "print(table_docs[10][\"text\"][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d72588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross page overlap\n",
    "\n",
    "def add_cross_page_overlap(pages, tail_chars=400):\n",
    "    \"\"\"\n",
    "    For each page, prepend last N chars from previous page text.\n",
    "    Keeps metadata the same (source/page).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_tail = \"\"\n",
    "    for p in pages:\n",
    "        merged_text = (prev_tail + \" \" + p[\"text\"]).strip()\n",
    "        prev_tail = p[\"text\"][-tail_chars:] if p[\"text\"] else \"\"\n",
    "        out.append({**p, \"text\": merged_text})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeebbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c46191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk page text\n",
    "def chunk_text(text, chunk_size=1200, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9fc5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chreate text chunks from pages\n",
    "def build_text_chunks(pages, chunk_size=1200, overlap=200, tail_chars=400):\n",
    "    pages_merged = add_cross_page_overlap(pages, tail_chars=tail_chars)\n",
    "\n",
    "    text_chunks = []\n",
    "    for p in pages_merged:\n",
    "        if not p[\"text\"]:\n",
    "            continue\n",
    "\n",
    "        splits = chunk_text(p[\"text\"], chunk_size=chunk_size, overlap=overlap)\n",
    "        for i, s in enumerate(splits):\n",
    "            text_chunks.append({\n",
    "                \"id\": f\"{p['source']}_p{p['page']}_text_c{i}\",\n",
    "                \"type\": \"text\",\n",
    "                \"company\": p[\"company\"],\n",
    "                \"year\": p[\"year\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"page\": p[\"page\"],\n",
    "                \"text\": s\n",
    "            })\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b8c12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table chunks\n",
    "\n",
    "def build_table_chunks(table_docs, max_chars=3500):\n",
    "    table_chunks = []\n",
    "    for t in table_docs:\n",
    "        txt = t[\"text\"].strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # If table text is short enough, keep as one chunk\n",
    "        if len(txt) <= max_chars:\n",
    "            table_chunks.append({\n",
    "                \"id\": t[\"id\"],\n",
    "                \"type\": \"table\",\n",
    "                \"company\": t[\"company\"],\n",
    "                \"year\": t[\"year\"],\n",
    "                \"source\": t[\"source\"],\n",
    "                \"page\": t[\"page\"],\n",
    "                \"text\": txt\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Else split table by lines into groups\n",
    "        lines = txt.splitlines()\n",
    "        buf, part = [], 0\n",
    "        cur_len = 0\n",
    "        for line in lines:\n",
    "            if cur_len + len(line) + 1 > max_chars and buf:\n",
    "                table_chunks.append({\n",
    "                    \"id\": f\"{t['id']}_part{part}\",\n",
    "                    \"type\": \"table\",\n",
    "                    \"company\": t[\"company\"],\n",
    "                    \"year\": t[\"year\"],\n",
    "                    \"source\": t[\"source\"],\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"text\": \"\\n\".join(buf)\n",
    "                })\n",
    "                buf, cur_len = [], 0\n",
    "                part += 1\n",
    "            buf.append(line)\n",
    "            cur_len += len(line) + 1\n",
    "\n",
    "        if buf:\n",
    "            table_chunks.append({\n",
    "                \"id\": f\"{t['id']}_part{part}\",\n",
    "                \"type\": \"table\",\n",
    "                \"company\": t[\"company\"],\n",
    "                \"year\": t[\"year\"],\n",
    "                \"source\": t[\"source\"],\n",
    "                \"page\": t[\"page\"],\n",
    "                \"text\": \"\\n\".join(buf)\n",
    "            })\n",
    "\n",
    "    return table_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5147418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunks: 1345\n",
      "Table chunks: 152\n",
      "Total chunks: 1497\n",
      "\n",
      "Sample text chunk:\n",
      " NOV_2023.pdf_p1_text_c0 | page 1\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington DC 20549\n",
      "FORM 10K\n",
      "Mark One\n",
      " ANNUAL REPORT PURSUANT TO SECTION 13 OR 15d OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September 30 2023\n",
      "or\n",
      " TRANSITION REPORT PURSUANT TO SECTION 13 OR 15d OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from              to             \n",
      "Commission File Number 00136743\n",
      "Ap\n",
      "\n",
      "Sample table chunk:\n",
      " NOV_2023.pdf_p3_t0 | page 3\n",
      "[TABLE]\n",
      "Company: Apple\n",
      "FilingYear: 2023\n",
      "Source: NOV_2023.pdf\n",
      "Page: 3\n",
      "Years: 2023\n",
      "\n",
      "Item 1. | Business | 1\n",
      "Item 1A. | Risk Factors | 5\n",
      "Item 1B. | Unresolved Staff Comments | 16\n",
      "Item 1C. | Cybersecurity | 16\n",
      "Item 2. | Properties | 17\n",
      "Item 3. | Legal Proceedings | 17\n",
      "Item 4. | Mine Safety Disclosures | 17\n"
     ]
    }
   ],
   "source": [
    "# final chunks\n",
    "\n",
    "text_chunks = build_text_chunks(pages, chunk_size=1200, overlap=200, tail_chars=400)\n",
    "table_chunks = build_table_chunks(table_docs, max_chars=3500)\n",
    "\n",
    "chunks = text_chunks + table_chunks\n",
    "\n",
    "print(\"Text chunks:\", len(text_chunks))\n",
    "print(\"Table chunks:\", len(table_chunks))\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "print(\"\\nSample text chunk:\\n\", chunks[0][\"id\"], \"| page\", chunks[0][\"page\"])\n",
    "print(chunks[0][\"text\"][:400])\n",
    "\n",
    "print(\"\\nSample table chunk:\\n\", table_chunks[0][\"id\"], \"| page\", table_chunks[0][\"page\"])\n",
    "print(table_chunks[0][\"text\"][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed632456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # folder created locally\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    return s[:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10b3ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_LOCAL_MODEL_CACHE = {}\n",
    "\n",
    "def _get_local_model(model_name: str):\n",
    "    if model_name in _LOCAL_MODEL_CACHE:\n",
    "        return _LOCAL_MODEL_CACHE[model_name]\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    m = SentenceTransformer(model_name, device=device)\n",
    "    _LOCAL_MODEL_CACHE[model_name] = m\n",
    "    return m\n",
    "\n",
    "def _is_e5(model_name: str) -> bool:\n",
    "    return \"e5\" in model_name.lower()\n",
    "\n",
    "def embed_texts(texts, cfg, *, mode=\"doc\"):\n",
    "    provider = cfg[\"provider\"].lower()\n",
    "\n",
    "    # ---- Local (GPU) ----\n",
    "    if provider == \"local\":\n",
    "        model = _get_local_model(cfg[\"model_name\"])\n",
    "        if _is_e5(cfg[\"model_name\"]):\n",
    "            prefix = \"query: \" if mode == \"query\" else \"doc: \"\n",
    "            texts = [prefix + t for t in texts]\n",
    "        vecs = model.encode(texts, batch_size=64, normalize_embeddings=True, show_progress_bar=False)\n",
    "        return vecs.tolist()\n",
    "\n",
    "\n",
    "# ---- Google (Gemini) ----\n",
    "    if provider == \"google\":\n",
    "        from google import genai\n",
    "        api_key = os.getenv(cfg[\"api_key_env\"])\n",
    "        if not api_key:\n",
    "            raise ValueError(f\"Missing env var: {cfg['api_key_env']}\")\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            r = client.models.embed_content(model=cfg[\"model_name\"], contents=t)\n",
    "            out.append(r.embeddings[0].values)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(f\"Unknown provider: {cfg['provider']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- OpenAI ----\n",
    "    '''\n",
    "    if provider == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        api_key = os.getenv(cfg[\"api_key_env\"])\n",
    "        if not api_key:\n",
    "            raise ValueError(f\"Missing env var: {cfg['api_key_env']}\")\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        resp = client.embeddings.create(model=cfg[\"model_name\"], input=texts)\n",
    "        return [d.embedding for d in resp.data]'''\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc92865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_chunks_to_chroma(chunks, cfg, batch_size=64):\n",
    "    # Collection name encodes provider+model+dim so no mismatch\n",
    "    collection_name = safe_name(\n",
    "        f\"{cfg['provider']}_{cfg['model_name']}_{cfg['dimensions']}\"\n",
    "    )\n",
    "\n",
    "    # Create or get collection (use cosine space)\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    # Add in batches\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        texts = [c[\"text\"] for c in batch]\n",
    "        ids = [c[\"id\"] for c in batch]\n",
    "\n",
    "        # store only JSON-serializable metadata\n",
    "        metadatas = [{\n",
    "            \"type\": c[\"type\"],\n",
    "            \"company\": c[\"company\"],\n",
    "            \"year\": str(c[\"year\"]),\n",
    "            \"source\": c[\"source\"],\n",
    "            \"page\": int(c[\"page\"]),\n",
    "        } for c in batch]\n",
    "\n",
    "        embs = embed_texts(texts, cfg, mode=\"doc\")\n",
    "\n",
    "        # sanity check: dimension match\n",
    "        got_dim = len(embs[0])\n",
    "        if got_dim != cfg[\"dimensions\"]:\n",
    "            raise ValueError(\n",
    "                f\"Dim mismatch for {cfg['provider']} {cfg['model_name']}: \"\n",
    "                f\"expected {cfg['dimensions']}, got {got_dim}\"\n",
    "            )\n",
    "\n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas,\n",
    "            embeddings=embs\n",
    "        )\n",
    "\n",
    "    return collection_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea209f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed: google_text_embedding_004_768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed: local_all_minilm_l6_v2_384\n",
      "Indexed: local_intfloat_e5_large_v2_1024\n",
      "\n",
      "All collections created:\n",
      " - google_text_embedding_004_768\n",
      " - local_all_minilm_l6_v2_384\n",
      " - local_intfloat_e5_large_v2_1024\n"
     ]
    }
   ],
   "source": [
    "created = []\n",
    "for cfg in embedding_models:\n",
    "    name = upsert_chunks_to_chroma(chunks, cfg, batch_size=64)\n",
    "    created.append(name)\n",
    "    print(\"Indexed:\", name)\n",
    "\n",
    "print(\"\\nAll collections created:\")\n",
    "for n in created:\n",
    "    print(\" -\", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a974546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_for_model(cfg):\n",
    "    collection_name = safe_name(f\"{cfg['provider']}_{cfg['model_name']}_{cfg['dimensions']}\")\n",
    "    return chroma_client.get_collection(name=collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "240cf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_topk_with_cfg(collection, query, cfg, k=8, where=None):\n",
    "    q_emb = embed_texts([query], cfg, mode=\"query\")[0]  # correct dims\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        where=where,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    hits = []\n",
    "    for _id, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        hits.append({\"id\": _id, \"text\": doc, \"meta\": meta, \"distance\": dist})\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa670144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_text_chunk_id(chunk_id):\n",
    "    \"\"\"\n",
    "    Extract (source, page, chunk_index) from ids like:\n",
    "    Tesla_2023.pdf_p32_text_c3\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_p(\\d+)_text_c(\\d+)$\", chunk_id)\n",
    "    if not m:\n",
    "        return None\n",
    "    return (m.group(1), int(m.group(2)), int(m.group(3)))\n",
    "\n",
    "def expand_neighbors(collection, hits, window=1, max_neighbors=30):\n",
    "    \"\"\"\n",
    "    For each hit, include +/- window neighboring chunks if they exist.\n",
    "    Only applies to text chunks with _text_cX ids.\n",
    "    \"\"\"\n",
    "    wanted_ids = set(h[\"id\"] for h in hits)\n",
    "\n",
    "    for h in hits:\n",
    "        parsed = parse_text_chunk_id(h[\"id\"])\n",
    "        if not parsed:\n",
    "            continue\n",
    "        source, page, cidx = parsed\n",
    "        for j in range(cidx - window, cidx + window + 1):\n",
    "            if j < 0:\n",
    "                continue\n",
    "            nid = f\"{source}_p{page}_text_c{j}\"\n",
    "            wanted_ids.add(nid)\n",
    "            if len(wanted_ids) >= max_neighbors + len(hits):\n",
    "                break\n",
    "\n",
    "    # fetch neighbor docs by ids (get)\n",
    "    got = collection.get(ids=list(wanted_ids), include=[\"documents\", \"metadatas\"])\n",
    "    out = []\n",
    "    for _id, doc, meta in zip(got[\"ids\"], got[\"documents\"], got[\"metadatas\"]):\n",
    "        out.append({\"id\": _id, \"text\": doc, \"meta\": meta})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39d5a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_and_sort(hits):\n",
    "    seen = {}\n",
    "    for h in hits:\n",
    "        seen[h[\"id\"]] = h\n",
    "\n",
    "    final = list(seen.values())\n",
    "    final.sort(key=lambda x: (\n",
    "        x[\"meta\"].get(\"source\", \"\"),\n",
    "        int(x[\"meta\"].get(\"page\", 0)),\n",
    "        x[\"meta\"].get(\"type\", \"\"),\n",
    "        x[\"id\"]\n",
    "    ))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26c4167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_pack(chunks):\n",
    "    \"\"\"\n",
    "    Returns a single context string + a citation list.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    citations = []\n",
    "    for c in chunks:\n",
    "        src = c[\"meta\"].get(\"source\")\n",
    "        page = c[\"meta\"].get(\"page\")\n",
    "        ctype = c[\"meta\"].get(\"type\")\n",
    "        blocks.append(\n",
    "            f\"[{ctype.upper()}] {src} (page {page})\\n{c['text']}\".strip()\n",
    "        )\n",
    "        citations.append({\"id\": c[\"id\"], \"source\": src, \"page\": page, \"type\": ctype})\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(blocks)\n",
    "    return context, citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a75798d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncfg = embedding_models[1]  # Google text-embedding-004 (for example)\\ncollection = get_collection_for_model(cfg)\\n\\nquestion = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\\nhits = retrieve_topk(collection, question, k=8)\\n\\n# expand neighbors (helps continuity)\\nexpanded = expand_neighbors(collection, hits, window=1)\\n\\nfinal_chunks = dedupe_and_sort(hits + expanded)\\ncontext, citations = build_context_pack(final_chunks)\\n\\nprint(\"Retrieved chunks:\", len(final_chunks))\\nprint(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\\nprint(context[:1200])\\n\\nprint(\"\\n--- CITATIONS ---\")\\nprint(citations[:10])\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cfg = embedding_models[1]  # Google text-embedding-004 (for example)\n",
    "collection = get_collection_for_model(cfg)\n",
    "\n",
    "question = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\n",
    "hits = retrieve_topk(collection, question, k=8)\n",
    "\n",
    "# expand neighbors (helps continuity)\n",
    "expanded = expand_neighbors(collection, hits, window=1)\n",
    "\n",
    "final_chunks = dedupe_and_sort(hits + expanded)\n",
    "context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "print(\"Retrieved chunks:\", len(final_chunks))\n",
    "print(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\n",
    "print(context[:1200])\n",
    "\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(citations[:10])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8990139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'provider': 'Local', 'model_name': 'all-MiniLM-L6-v2', 'api_key_env': None, 'dimensions': 384}\n",
      "Retrieved chunks: 11\n",
      "\n",
      "--- CONTEXT (first 1200 chars) ---\n",
      "\n",
      "[TEXT] NOV_2023.pdf (page 24)\n",
      "Company repurchased 766 billion of its common stock and paid dividends and dividend equivalents of 150 billion\n",
      "Macroeconomic Conditions\n",
      "Macroeconomic conditions including inflation changes in interest rates and currency fluctuations have directly and indirectly impacted and could in the future\n",
      "materially impact the Companys results of operations and financial condition\n",
      "Apple Inc  2023 Form 10K  20 Segment Operating Performance\n",
      "The following table shows net sales by reportable segment for 2023 2022 and 2021 dollars in millions\n",
      "2023\n",
      "Change\n",
      "2022\n",
      "Change\n",
      "2021\n",
      "Net sales by reportable segment\n",
      "Americas\n",
      "\n",
      "162560 \n",
      "4\n",
      "\n",
      "169658 \n",
      "11 \n",
      "\n",
      "153306 \n",
      "Europe\n",
      "94294 \n",
      "1\n",
      "95118 \n",
      "7 \n",
      "89307 \n",
      "Greater China\n",
      "72559 \n",
      "2\n",
      "74200 \n",
      "9 \n",
      "68366 \n",
      "Japan\n",
      "24257 \n",
      "7\n",
      "25977 \n",
      "9\n",
      "28482 \n",
      "Rest of Asia Pacific\n",
      "29615 \n",
      "1 \n",
      "29375 \n",
      "11 \n",
      "26356 \n",
      "Total net sales\n",
      "\n",
      "383285 \n",
      "3\n",
      "\n",
      "394328 \n",
      "8 \n",
      "\n",
      "365817 \n",
      "Americas\n",
      "Americas net sales decreased 4 or 71 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac partially offset by higher net sales of\n",
      "Services\n",
      "Europe\n",
      "Europe net sales decreased 1 or 824 million during 2023 compared to 2022 The weakness in foreign currencies relative to the US dollar accounted for\n",
      "more t\n",
      "\n",
      "--- CITATIONS ---\n",
      "[{'id': 'NOV_2023.pdf_p24_text_c0', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p24_text_c1', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p24_text_c2', 'source': 'NOV_2023.pdf', 'page': 24, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_t0', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'table'}, {'id': 'NOV_2023.pdf_p26_t1', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'table'}, {'id': 'NOV_2023.pdf_p26_text_c0', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_text_c1', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p26_text_c2', 'source': 'NOV_2023.pdf', 'page': 26, 'type': 'text'}, {'id': 'NOV_2023.pdf_p38_text_c0', 'source': 'NOV_2023.pdf', 'page': 38, 'type': 'text'}, {'id': 'NOV_2023.pdf_p38_text_c1', 'source': 'NOV_2023.pdf', 'page': 38, 'type': 'text'}]\n"
     ]
    }
   ],
   "source": [
    "cfg = embedding_models[1]\n",
    "print(cfg)  # Google text-embedding-004\n",
    "collection = get_collection_for_model(cfg)\n",
    "\n",
    "question = \"What was the total gross margin in 2023 vs 2022 vs 2021?\"\n",
    "hits = retrieve_topk_with_cfg(collection, question, cfg, k=8, where={\"company\":\"Apple\"})  \n",
    "\n",
    "expanded = expand_neighbors(collection, hits, window=1)\n",
    "\n",
    "final_chunks = dedupe_and_sort(hits + expanded)\n",
    "context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "print(\"Retrieved chunks:\", len(final_chunks))\n",
    "print(\"\\n--- CONTEXT (first 1200 chars) ---\\n\")\n",
    "print(context[:1200])\n",
    "\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(citations[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bfce2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are a financial filing assistant.\n",
    "\n",
    "RULES:\n",
    "- Use ONLY the provided context.\n",
    "- If the answer is not in the context, say: \"I don't have enough information in the provided context.\"\n",
    "- For every numeric claim, include a citation in this format: (Source: <file>, Page: <page>).\n",
    "- Keep the answer concise and structured.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a776f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rushy\\anaconda3\\envs\\finbot-env\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.17) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total gross margin was:\n",
      "*   **2023:** $169,148 million (Source: NOV_2023.pdf, Page: 26)\n",
      "*   **2022:** $170,782 million (Source: NOV_2023.pdf, Page: 26)\n",
      "*   **2021:** $152,836 million (Source: NOV_2023.pdf, Page: 26)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import time\n",
    "from google.api_core.exceptions import InternalServerError, ServiceUnavailable\n",
    "\n",
    "RETRYABLE_ERRORS = (InternalServerError, ServiceUnavailable)\n",
    "\n",
    "'''\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts= 5,\n",
    "    exp_base = 0.2,\n",
    "    initial_delay = 0.5,\n",
    "    http_status_codes = [500, 502, 503, 504],   \n",
    ")\n",
    "'''\n",
    "\n",
    "def generate_answer_gemini(\n",
    "    question,\n",
    "    context,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    max_retries=5,\n",
    "    backoff=0.5\n",
    "):\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing GEMINI_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    prompt = build_rag_prompt(question, context)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=prompt\n",
    "            )\n",
    "            return resp.text\n",
    "\n",
    "        except RETRYABLE_ERRORS as e:\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            sleep_time = backoff * (2 ** (attempt - 1))\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "answer = generate_answer_gemini(question, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9f8a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RUNS = [\n",
    "  {\"name\":\"google_768\", \"cfg\": embedding_models[0], \"collection\":\"google_text_embedding_004_768\"},\n",
    "  {\"name\":\"minilm_384\", \"cfg\": embedding_models[1], \"collection\":\"local_all_minilm_l6_v2_384\"},\n",
    "  {\"name\":\"e5_1024\",    \"cfg\": embedding_models[2], \"collection\":\"local_intfloat_e5_large_v2_1024\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14edc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "  {\"id\":\"q1\", \"company\":\"Apple\", \"question\":\"What was total gross margin in 2023 vs 2022 vs 2021?\"},\n",
    "  {\"id\":\"q2\", \"company\":\"Apple\", \"question\":\"What are the main risk factors mentioned about supply chain?\"},\n",
    "  {\"id\":\"q3\", \"company\":\"Tesla\", \"question\":\"What does Tesla say about revenue recognition?\"},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb26c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_rag_json_prompt(question, context):\n",
    "    schema = {\n",
    "      \"question\": \"string\",\n",
    "      \"answer\": \"string\",\n",
    "      \"citations\": [{\"source\":\"string\", \"page\": \"number\", \"type\":\"text|table\"}],\n",
    "      \"support_level\": \"high|medium|low\",\n",
    "      \"missing_info\": \"string|null\"\n",
    "    }\n",
    "    return f\"\"\"\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the provided context.\n",
    "- If not enough info, set support_level=\"low\" and missing_info with what is missing.\n",
    "- Every numeric claim must have citations with correct source+page.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad1e594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "from google.api_core.exceptions import InternalServerError, ServiceUnavailable\n",
    "\n",
    "RETRYABLE_ERRORS = (InternalServerError, ServiceUnavailable)\n",
    "\n",
    "def generate_answer_gemini_json(\n",
    "    question,\n",
    "    context,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    max_retries=5,\n",
    "    backoff=0.5\n",
    "):\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing GEMINI_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    # IMPORTANT: your prompt must explicitly instruct JSON-only output\n",
    "    prompt = build_rag_json_prompt(question, context)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=prompt\n",
    "            )\n",
    "\n",
    "            txt = resp.text.strip()\n",
    "\n",
    "            # ✅ Strict JSON parse attempt\n",
    "            try:\n",
    "                return json.loads(txt)\n",
    "            except json.JSONDecodeError:\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": txt,\n",
    "                    \"citations\": [],\n",
    "                    \"support_level\": \"low\",\n",
    "                    \"missing_info\": \"Model did not return valid JSON\"\n",
    "                }\n",
    "\n",
    "        except RETRYABLE_ERRORS as e:\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            sleep_time = backoff * (2 ** (attempt - 1))\n",
    "            time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15d5ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def p50(xs): return float(np.percentile(xs, 50)) if xs else None\n",
    "def p90(xs): return float(np.percentile(xs, 90)) if xs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdcb6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def estimate_citation_coverage(answer_text: str):\n",
    "    # counts \"(Source:\" occurrences as citations\n",
    "    cites = len(re.findall(r\"\\(Source:\", answer_text))\n",
    "    # rough sentence count\n",
    "    sents = max(1, len(re.findall(r\"[.!?]\\s+\", answer_text)) + 1)\n",
    "    return min(1.0, cites / sents)\n",
    "\n",
    "def support_level_to_score(level: str):\n",
    "    level = (level or \"\").lower()\n",
    "    return {\"high\": 1.0, \"medium\": 0.6, \"low\": 0.2}.get(level, 0.2)\n",
    "\n",
    "def retrieval_score_from_hits(hits):\n",
    "    # use best distance only; if missing, fallback\n",
    "    dists = [h.get(\"distance\") for h in hits if h.get(\"distance\") is not None]\n",
    "    if not dists:\n",
    "        return 0.5\n",
    "    best = min(dists)\n",
    "    # turn distance into a bounded score (tunable)\n",
    "    return float(1 / (1 + best))\n",
    "\n",
    "def confidence_score(hits, gen_json):\n",
    "    ans = gen_json.get(\"answer\", \"\")\n",
    "    cov = estimate_citation_coverage(ans)\n",
    "    sup = support_level_to_score(gen_json.get(\"support_level\"))\n",
    "\n",
    "    rscore = retrieval_score_from_hits(hits)\n",
    "\n",
    "    # weighted composite\n",
    "    return round(0.45*rscore + 0.35*cov + 0.20*sup, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e4917a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RagChat:\n",
    "    def __init__(self, max_turns=6):\n",
    "        self.history = deque(maxlen=max_turns)\n",
    "\n",
    "    def add_turn(self, q, a):\n",
    "        self.history.append({\"question\": q, \"answer\": a})\n",
    "\n",
    "    def history_block(self):\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        items = []\n",
    "        for t in self.history:\n",
    "            items.append(f\"User: {t['question']}\\nAssistant: {t['answer']}\")\n",
    "        return \"\\n\\n\".join(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d46db50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_json_prompt_with_history(question, context, history_text):\n",
    "    base = build_rag_json_prompt(question, context)\n",
    "    if history_text.strip():\n",
    "        return f\"HISTORY:\\n{history_text}\\n\\n{base}\"\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27426a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_question(model_run, question_obj, *, k=8):\n",
    "    cfg = model_run[\"cfg\"]\n",
    "    collection = chroma_client.get_collection(model_run[\"collection\"])\n",
    "\n",
    "    question = question_obj[\"question\"]\n",
    "    where = {\"company\": question_obj[\"company\"]} if \"company\" in question_obj else None\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    hits = retrieve_topk_with_cfg(collection, question, cfg, k=k, where=where)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    expanded = expand_neighbors(collection, hits, window=1)\n",
    "    final_chunks = dedupe_and_sort(hits + expanded)\n",
    "    context, citations = build_context_pack(final_chunks)\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    gen = generate_answer_gemini_json(question, context)\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    # compute confidence\n",
    "    conf = confidence_score(hits, gen)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_run[\"name\"],\n",
    "        \"question_id\": question_obj.get(\"id\"),\n",
    "        \"company\": question_obj.get(\"company\"),\n",
    "        \"question\": question,\n",
    "        \"retrieval_ms\": round((t1 - t0) * 1000, 2),\n",
    "        \"generation_ms\": round((t3 - t2) * 1000, 2),\n",
    "        \"end_to_end_ms\": round((t3 - t0) * 1000, 2),\n",
    "        \"confidence\": conf,\n",
    "        \"response_json\": gen,\n",
    "        \"top_sources\": list({(c[\"source\"], c[\"page\"]) for c in citations})[:6],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38c11dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9] google_768 | What was total gross margin in 2023 vs 2022 vs 202...\n",
      "[2/9] minilm_384 | What was total gross margin in 2023 vs 2022 vs 202...\n",
      "[3/9] e5_1024 | What was total gross margin in 2023 vs 2022 vs 202...\n",
      "[4/9] google_768 | What are the main risk factors mentioned about sup...\n",
      "[5/9] minilm_384 | What are the main risk factors mentioned about sup...\n",
      "[6/9] e5_1024 | What are the main risk factors mentioned about sup...\n",
      "[7/9] google_768 | What does Tesla say about revenue recognition?...\n",
      "[8/9] minilm_384 | What does Tesla say about revenue recognition?...\n",
      "[9/9] e5_1024 | What does Tesla say about revenue recognition?...\n",
      "{'google_768': {'n': 3, 'confidence_avg': 0.3876, 'retrieval_p50_ms': 704.5, 'retrieval_p90_ms': 1223.78, 'generation_p50_ms': 13832.16, 'generation_p90_ms': 15678.152, 'e2e_p50_ms': 14336.62, 'e2e_p90_ms': 16345.612000000001}, 'minilm_384': {'n': 3, 'confidence_avg': 0.3623, 'retrieval_p50_ms': 64.82, 'retrieval_p90_ms': 339.084, 'generation_p50_ms': 3704.92, 'generation_p90_ms': 19980.488, 'e2e_p50_ms': 4115.17, 'e2e_p90_ms': 20117.45}, 'e5_1024': {'n': 3, 'confidence_avg': 0.4261, 'retrieval_p50_ms': 286.56, 'retrieval_p90_ms': 536.72, 'generation_p50_ms': 16106.06, 'generation_p90_ms': 18864.644, 'e2e_p50_ms': 16396.02, 'e2e_p90_ms': 19039.26}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from google.genai.errors import ClientError\n",
    "\n",
    "@retry(\n",
    "    retry=retry_if_exception_type(ClientError),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    reraise=True\n",
    ")\n",
    "def run_one_with_retry(m, q, k=8):\n",
    "    return run_one_question(m, q, k=k)\n",
    "\n",
    "def benchmark(models, questions, delay_between=2.0):\n",
    "    results = []\n",
    "    total = len(questions) * len(models)\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        for j, m in enumerate(models):\n",
    "            idx = i * len(models) + j + 1\n",
    "            print(f\"[{idx}/{total}] {m['name']} | {q['question'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                r = run_one_with_retry(m, q, k=8)\n",
    "                results.append(r)\n",
    "            except ClientError as e:\n",
    "                print(f\"  ⚠ Skipped after retries: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # throttle to stay under RPM limits\n",
    "            time.sleep(delay_between)\n",
    "\n",
    "    # summarize latency percentiles by model\n",
    "    summary = {}\n",
    "    for m in models:\n",
    "        name = m[\"name\"]\n",
    "        rows = [r for r in results if r[\"model\"] == name]\n",
    "        retr = [r[\"retrieval_ms\"] for r in rows]\n",
    "        gen  = [r[\"generation_ms\"] for r in rows]\n",
    "        e2e  = [r[\"end_to_end_ms\"] for r in rows]\n",
    "        conf = [r[\"confidence\"] for r in rows]\n",
    "\n",
    "        summary[name] = {\n",
    "            \"n\": len(rows),\n",
    "            \"confidence_avg\": round(float(np.mean(conf)), 4) if conf else None,\n",
    "            \"retrieval_p50_ms\": p50(retr),\n",
    "            \"retrieval_p90_ms\": p90(retr),\n",
    "            \"generation_p50_ms\": p50(gen),\n",
    "            \"generation_p90_ms\": p90(gen),\n",
    "            \"e2e_p50_ms\": p50(e2e),\n",
    "            \"e2e_p90_ms\": p90(e2e),\n",
    "        }\n",
    "    return results, summary\n",
    "\n",
    "results, summary = benchmark(MODEL_RUNS, EVAL_QUESTIONS, delay_between=2.0)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173358bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
